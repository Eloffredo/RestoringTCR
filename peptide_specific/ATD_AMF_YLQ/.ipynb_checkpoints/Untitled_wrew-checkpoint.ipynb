{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c7285f-9be2-4961-9de2-802f1e2d3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9b151-76f7-4090-9943-c55076eaba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "\n",
    "    def __init__(self, L: int = 30):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels= 20, out_channels= 16, kernel_size= 1, padding = 'same')\n",
    "        self.conv3 = nn.Conv1d(in_channels= 20, out_channels= 16, kernel_size= 3, padding = 'same')\n",
    "        self.conv5 = nn.Conv1d(in_channels= 20, out_channels= 16, kernel_size= 5, padding = 'same')\n",
    "        self.conv7 = nn.Conv1d(in_channels= 20, out_channels= 16, kernel_size= 7, padding = 'same')\n",
    "        self.conv9 = nn.Conv1d(in_channels= 20, out_channels= 16, kernel_size= 9, padding = 'same')\n",
    "        self.batch1 = nn.BatchNorm1d(num_features= 16)\n",
    "        self.batch3 = nn.BatchNorm1d(num_features= 16)\n",
    "        self.batch5 = nn.BatchNorm1d(num_features= 16)\n",
    "        self.batch7 = nn.BatchNorm1d(num_features= 16)\n",
    "        self.batch9 = nn.BatchNorm1d(num_features= 16)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(in_features = L*16*5, out_features = 16, bias = True)\n",
    "        self.batch = nn.BatchNorm1d(num_features= 16)\n",
    "        self.dense2 = nn.Linear(in_features = 16, out_features = 4, bias = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.batch1( F.relu(self.conv1(x)) )\n",
    "        conv3 = self.batch3( F.relu(self.conv3(x)) )\n",
    "        conv5 = self.batch5( F.relu(self.conv5(x)) )\n",
    "        conv7 = self.batch7( F.relu(self.conv7(x)) )\n",
    "        conv9 = self.batch9( F.relu(self.conv9(x)) )\n",
    "\n",
    "        cat = self.flatten( torch.cat([conv1, conv3, conv5, conv7, conv9 ], dim =1) )\n",
    "        x = self.batch( F.relu(self.dense(cat)) )\n",
    "        x = F.softmax( self.dense2(x), dim = 1 )\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model: nn.Module, train_loader: DataLoader, num_epochs: int, device: str = \"cuda\", reweighting: bool = False) -> List[float]:\n",
    "    model.to(device)\n",
    "\n",
    "    if reweighting:\n",
    "        class_weights = (train_loader.dataset.labels.size()[0]/torch.argmax(train_loader.dataset.labels, dim = 1).unique(return_counts=True)[-1]).to(device)\n",
    "    else: \n",
    "        class_weights = torch.tensor([1.,1.,1.,1.]).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr= 0.001)\n",
    "    train_loss = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        curr_loss =  0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            #with torch.autocast(device_type = device, dtype=torch.bfloat16):\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.view(outputs.size() ))\n",
    "            #print(loss.dtype, outputs.dtype )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            curr_loss +=loss.item()\n",
    "                \n",
    "        train_loss.append( curr_loss/(len(train_loader)) )\n",
    "\n",
    "    return train_loss  \n",
    "\n",
    "def evaluate(model: nn.Module, eval_loader: DataLoader, device: str = \"cuda\") -> List[float]:\n",
    "    model.to(device)\n",
    "    eval_loss = []\n",
    "    \n",
    "    model.eval()\n",
    "    curr_loss =  0.0\n",
    "    for inputs, labels in eval_loader:\n",
    "        #with torch.autocast(device_type = device, dtype=torch.bfloat16):\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(outputs.size() ))\n",
    "        #print(loss.dtype, outputs.dtype )\n",
    "        \n",
    "        curr_loss +=loss.item()\n",
    "            \n",
    "    eval_loss.append( curr_loss/(len(eval_loader)) )\n",
    "\n",
    "    return eval_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0de15-351d-4f21-a533-2572f6188894",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = model(L = 30)\n",
    "sum([p.numel() for p in mdl.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d0869-a379-4ba4-a8b3-f0c02cab6060",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class dataset_spec(Dataset):\n",
    "    def __init__(self, inputs,labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "def enc_list_bl_max_len(aa_seqs, blosum, max_seq_len):\n",
    "    # encode sequences:\n",
    "    sequences=[]\n",
    "    for seq in aa_seqs:\n",
    "        e_seq=np.zeros((len(seq),len(blosum[\"A\"])))\n",
    "        count=0\n",
    "        for aa in seq:\n",
    "            if aa in blosum:\n",
    "                e_seq[count]=blosum[aa]\n",
    "                count+=1\n",
    "            else:\n",
    "                print(aa)\n",
    "                sys.stderr.write(\"Unknown amino acid in peptides: \"+ aa +\", encoding aborted!\\n\")\n",
    "                sys.exit(2)\n",
    "                \n",
    "        sequences.append(e_seq)\n",
    "\n",
    "    # pad sequences:\n",
    "    #max_seq_len = max([len(x) for x in aa_seqs])\n",
    "    n_seqs = len(aa_seqs)\n",
    "    n_features = sequences[0].shape[1]\n",
    "\n",
    "    enc_aa_seq = np.zeros((n_seqs, max_seq_len, n_features))\n",
    "    for i in range(0,n_seqs):\n",
    "        enc_aa_seq[i, :sequences[i].shape[0], :n_features] = sequences[i]\n",
    "\n",
    "    return enc_aa_seq\n",
    "\n",
    "\n",
    "blosum50_20aa = {\n",
    "        'A': np.array((5,-2,-1,-2,-1,-1,-1,0,-2,-1,-2,-1,-1,-3,-1,1,0,-3,-2,0)),\n",
    "        'R': np.array((-2,7,-1,-2,-4,1,0,-3,0,-4,-3,3,-2,-3,-3,-1,-1,-3,-1,-3)),\n",
    "        'N': np.array((-1,-1,7,2,-2,0,0,0,1,-3,-4,0,-2,-4,-2,1,0,-4,-2,-3)),\n",
    "        'D': np.array((-2,-2,2,8,-4,0,2,-1,-1,-4,-4,-1,-4,-5,-1,0,-1,-5,-3,-4)),\n",
    "        'C': np.array((-1,-4,-2,-4,13,-3,-3,-3,-3,-2,-2,-3,-2,-2,-4,-1,-1,-5,-3,-1)),\n",
    "        'Q': np.array((-1,1,0,0,-3,7,2,-2,1,-3,-2,2,0,-4,-1,0,-1,-1,-1,-3)),\n",
    "        'E': np.array((-1,0,0,2,-3,2,6,-3,0,-4,-3,1,-2,-3,-1,-1,-1,-3,-2,-3)),\n",
    "        'G': np.array((0,-3,0,-1,-3,-2,-3,8,-2,-4,-4,-2,-3,-4,-2,0,-2,-3,-3,-4)),\n",
    "        'H': np.array((-2,0,1,-1,-3,1,0,-2,10,-4,-3,0,-1,-1,-2,-1,-2,-3,2,-4)),\n",
    "        'I': np.array((-1,-4,-3,-4,-2,-3,-4,-4,-4,5,2,-3,2,0,-3,-3,-1,-3,-1,4)),\n",
    "        'L': np.array((-2,-3,-4,-4,-2,-2,-3,-4,-3,2,5,-3,3,1,-4,-3,-1,-2,-1,1)),\n",
    "        'K': np.array((-1,3,0,-1,-3,2,1,-2,0,-3,-3,6,-2,-4,-1,0,-1,-3,-2,-3)),\n",
    "        'M': np.array((-1,-2,-2,-4,-2,0,-2,-3,-1,2,3,-2,7,0,-3,-2,-1,-1,0,1)),\n",
    "        'F': np.array((-3,-3,-4,-5,-2,-4,-3,-4,-1,0,1,-4,0,8,-4,-3,-2,1,4,-1)),\n",
    "        'P': np.array((-1,-3,-2,-1,-4,-1,-1,-2,-2,-3,-4,-1,-3,-4,10,-1,-1,-4,-3,-3)),\n",
    "        'S': np.array((1,-1,1,0,-1,0,-1,0,-1,-3,-3,0,-2,-3,-1,5,2,-4,-2,-2)),\n",
    "        'T': np.array((0,-1,0,-1,-1,-1,-1,-2,-2,-1,-1,-1,-1,-2,-1,2,5,-3,-2,0)),\n",
    "        'W': np.array((-3,-3,-4,-5,-5,-1,-3,-3,-3,-3,-2,-3,-1,1,-4,-4,-3,15,2,-3)),\n",
    "        'Y': np.array((-2,-1,-2,-3,-3,-1,-2,-3,2,-1,-1,-2,0,4,-3,-2,-2,2,8,-1)),\n",
    "        'V': np.array((0,-3,-3,-4,-1,-3,-3,-4,-4,4,1,-3,1,-1,-3,-2,0,-3,-1,5))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4bca5-2db2-4661-bad3-401179be29ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_samples = int(600)\n",
    "L =25; N_samples = int(250000)\n",
    "EPOCHS, batch_size = 5, 2048\n",
    "repeats = int(25)\n",
    "device = \"cuda\"\n",
    "\n",
    "peptide1, peptide2, peptide3 = 'AMFWSVPTV','YLQPRTFLL','ATDALMTGY'\n",
    "check1, check2, check3 = 1500,2100,170\n",
    "\n",
    "df1_1 = pd.read_csv(f'./Binders_{peptide1}.csv').drop_duplicates()\n",
    "df2 = pd.read_csv(f'./Generated_binders_{peptide1}_BERT_pmask{pm}_beta{beta:.2f}_finetuned_wcheckpoint{check1}.csv').drop_duplicates()\n",
    "df2 = df2[df2['CDR3b'].str.len()>7]\n",
    "df2 = df2[~df2['CDR3b'].isin(df1_1['CDR3b'])].dropna()\n",
    "\n",
    "data_pos_1 = pd.concat((df1_1.sample(50),df2.sample(51 - 50 ) )).drop_duplicates()\n",
    "data_pos_1['labels'] = 0\n",
    "df1_1['labels'] = 0\n",
    "\n",
    "data_pos_2 = pd.read_csv(f'./tchard_{peptide2}.csv').drop_duplicates()\n",
    "data_pos_2['labels'] = 1\n",
    "\n",
    "df1_3 = pd.read_csv(f'./Binders_{peptide3}.csv').drop_duplicates()\n",
    "df2 = pd.read_csv(f'./Generated_binders_{peptide3}_BERT_pmask{pm}_beta{beta:.2f}_finetuned_wcheckpoint{check3}.csv').drop_duplicates()\n",
    "df2 = df2[df2['CDR3b'].str.len()>7]\n",
    "df2 = df2[~df2['CDR3b'].isin(df1_3['CDR3b'])].dropna()\n",
    "\n",
    "data_pos_3 = pd.concat((df1_3.sample(120),df2.sample(121 - 120) )).drop_duplicates()\n",
    "data_pos_3['labels'] = 2\n",
    "df1_3['labels'] = 2\n",
    "\n",
    "T = int(24)\n",
    "\n",
    "results = []\n",
    "data_neg = pd.read_csv('./Background_notaligned.csv')\n",
    "data_neg['labels'] = 3\n",
    "\n",
    "Test_insample = []; Test_ext = []\n",
    "print('Training... and testing over %d repeats with entry-state: ' %(repeats))\n",
    "\n",
    "df_tchar = pd.read_csv(f'./tchard_{peptide1}.csv').drop_duplicates();\n",
    "ext_eval_1 = df_tchar[~df_tchar['CDR3b'].isin(data_pos_1['CDR3b'])]\n",
    "ext_eval_1['labels']=0\n",
    "\n",
    "df_tchar = pd.read_csv(f'./Binders_{peptide2}.csv').drop_duplicates();\n",
    "ext_eval_2 = df_tchar[~df_tchar['CDR3b'].isin(data_pos_2['CDR3b'])]\n",
    "ext_eval_2['labels']=1\n",
    "\n",
    "df_tchar = pd.read_csv(f'./tchard_{peptide3}.csv').drop_duplicates();\n",
    "ext_eval_3 = df_tchar[~df_tchar['CDR3b'].isin(data_pos_3['CDR3b'])]\n",
    "ext_eval_3['labels']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4bb9de-3d42-432d-acf9-85c1bed3ac06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test_insample = []; Test_ext = []\n",
    "\n",
    "for K in range(repeats):  \n",
    "    if (K == 0 or K == repeats-1):\n",
    "        print('\\t \\t repeat n: %d' %K)\n",
    "    \n",
    "    data_pos_in_1 = data_pos_1.sample(51)\n",
    "    data_pos_in_2 = data_pos_2.sample(P_samples)\n",
    "    data_pos_in_3 = data_pos_3.sample(121)\n",
    "    data_neg_in = data_neg.sample(N_samples)\n",
    "    \n",
    "    \n",
    "    cdr_in = pd.concat((data_pos_in_1,data_pos_in_2,data_pos_in_3,data_neg_in ))\n",
    "    cdr_in.reset_index(inplace=True,drop=True)\n",
    "    y_train = cdr_in['labels']; y_train = F.one_hot(torch.tensor(y_train), num_classes = 4) \n",
    "    cdr_in = enc_list_bl_max_len(cdr_in['CDR3b'], blosum50_20aa, L)\n",
    "\n",
    "    train_dataset = dataset_spec( torch.tensor(cdr_in.transpose(0,2,1),dtype = torch.float32 ), y_train )\n",
    "    train_loader = DataLoader( train_dataset, batch_size=batch_size, shuffle=True )\n",
    "\n",
    "    mdl = model(L = L)\n",
    "    train_tmp = train(mdl, train_loader, num_epochs = EPOCHS)\n",
    "    with torch.no_grad():\n",
    "        mdl.eval()\n",
    "\n",
    "        pos_test_1 = df1_1[~df1_1['CDR3b'].isin(data_pos_in_1['CDR3b'])].dropna().sample(T)\n",
    "        pos_test_2 = data_pos_2[~data_pos_2['CDR3b'].isin(data_pos_in_2['CDR3b'])].dropna().sample(T)\n",
    "        pos_test_3 = df1_3[~df1_3['CDR3b'].isin(data_pos_in_3['CDR3b'])].dropna().sample(T)\n",
    "        neg_test = data_neg[~data_neg.isin(data_neg_in)].dropna()\n",
    "        cdr_test = pd.concat((pos_test_1,pos_test_2,pos_test_3,neg_test.sample(T) ))\n",
    "        cdr_test.reset_index(inplace=True,drop=True)\n",
    "        \n",
    "        y_test = cdr_test['labels'];\n",
    "        cdr_test = enc_list_bl_max_len(cdr_test['CDR3b'], blosum50_20aa, L)\n",
    "        output = mdl(torch.tensor(cdr_test.transpose(0,2,1), dtype= torch.float32).to(device) )\n",
    "        auc = roc_auc_score(y_score= output.detach().cpu().numpy(), y_true = y_test, multi_class= 'ovo')  \n",
    "        acc = accuracy_score(y_pred= np.argmax(output.detach().cpu().numpy(),axis = 1), y_true = y_test)        \n",
    "        Test_insample.append([auc, acc])\n",
    "        \n",
    "        neg_test = data_neg[~data_neg.isin(data_neg_in)].dropna().sample(2*T)\n",
    "        cdr_test_ext = pd.concat((ext_eval_1.sample(2*T),ext_eval_2.sample(2*T),ext_eval_3.sample(2*T),neg_test ))\n",
    "        cdr_test_ext.reset_index(inplace=True,drop=True)\n",
    "        y_test_ext = cdr_test_ext['labels']; #y_test_ext = F.one_hot(torch.tensor(y_test_ext.astype(np.int64)), num_classes = 4)\n",
    "        cdr_test_ext = enc_list_bl_max_len(cdr_test_ext['CDR3b'], blosum50_20aa, L)\n",
    "        \n",
    "        output = mdl(torch.tensor(cdr_test_ext.transpose(0,2,1), dtype= torch.float32).to(device) )\n",
    "        auc = roc_auc_score(y_score= output.detach().cpu().numpy(), y_true = y_test_ext, multi_class= 'ovo')  \n",
    "        acc = accuracy_score(y_pred= np.argmax(output.detach().cpu().numpy(),axis = 1), y_true = y_test_ext) \n",
    "        print(acc)\n",
    "        Test_ext.append([auc, acc])\n",
    "\n",
    "    del mdl\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc9eb3-1404-42b1-a9b3-f21a7bb661b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tback_n=np.mean(np.array(Test_insample),axis=0); ETback_n= np.std(np.array(Test_insample),axis=0)\n",
    "Text_n=np.mean(np.array(Test_ext),axis=0); EText_n= np.std(np.array(Test_ext),axis=0)\n",
    "\n",
    "results.append(np.concatenate(([P_samples],[N_samples],Tback_n,ETback_n,Text_n,EText_n )) )\n",
    "\n",
    "columns_name = ['Psamples','Nsamples','AUC-back','ACC-back','std-back','stdACC-back','AUC-ext','ACC-ext','std-ext','stdACC-ext']\n",
    "\n",
    "df = pd.DataFrame(results,columns=columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3cf857-abab-4b50-9a09-a55809fe3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./TMP_wrew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad75336-807a-47a4-93e0-03897f2f6490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030eda4-ef9a-4107-b0a2-6379ccbb9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93500248-820e-4f7f-a033-42c593999394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
